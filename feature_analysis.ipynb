{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1437ff303d01b9bab741635e72177a1dec786421"
   },
   "source": [
    "**Linear regression for descriptor analysis**\n\nWhich descriptors may be relevant to predict the performance of the schools in terms of registration and participation of the students? Whereas registration and participation are two possible targets which we want to improve (possibly in different ways), we will focus on determining which school metrics can best predict the number of registrants! This analysis can easiliy modified and applied also to other related questions.\n\nIn order to address this question, we will first pre-process the data, take the numerical features and then run LASSO and feature correlation analysis.\nThat is:\n\nstep 1: Data pre-processing and feature matching.\n\nstep 2: Finding relevant descriptors to predict school performance in terms of registrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from decimal import Decimal\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import rc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "c1e2ea5b865703ad40f581916a1d6ccf9e11ce06"
   },
   "outputs": [],
   "source": [
    "# function file to data frame\n",
    "def file_to_df(file):\n",
    "    filename, file_extension = os.path.splitext(file)\n",
    "    if file_extension=='.csv':\n",
    "        df = pd.read_csv(file, sep=',', header=0)\n",
    "    elif file_extension=='.tsv':\n",
    "        df = pd.read_csv(file, sep='\\t', header=0)\n",
    "    else:\n",
    "        print('Please provide csv or tsv file format.')\n",
    "    return df\n",
    "\n",
    "df_features_init = file_to_df('../input/data-science-for-good/2016 School Explorer.csv')\n",
    "df_target_init = file_to_df('../input/augmented-d5-shsat-2/augmented_D5_SHSAT_Registrations_and_Testers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "9043260bbe7ce2ae399b670b7fe7bdf57701cd13"
   },
   "outputs": [],
   "source": [
    "# subset to numeric only plus school ID\n",
    "df_features_init.rename(columns={'Location Code':'DBN'}, inplace=True)\n",
    "colnames = list(df_features_init)\n",
    "\n",
    "df_features = pd.DataFrame()\n",
    "df_features['DBN'] = df_features_init['DBN']\n",
    "for i in range(0,len(colnames)):\n",
    "    if is_numeric_dtype(df_features_init[colnames[i]]):\n",
    "        df_features[colnames[i]] = df_features_init[colnames[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "4c2e4d01837cb1832d2a1a1fd441b61848ca1338"
   },
   "outputs": [],
   "source": [
    "# match features and targets\n",
    "DBN_list = df_target_init['DBN'].unique()\n",
    "df_features = df_features[df_features['DBN'].isin(DBN_list)]\n",
    "\n",
    "DBN_list = df_features['DBN'].unique()\n",
    "df_target = df_target_init.copy()\n",
    "df_target = df_target[df_target['DBN'].isin(DBN_list)]\n",
    "\n",
    "# still not fitting, due to year inconsistencies\n",
    "print(df_features.shape)\n",
    "print(df_target.shape)\n",
    "print(df_target.shape[0]/df_features.shape[0]*1.)\n",
    "df_var = df_target.sort_values(by=['DBN'],ascending=True)\n",
    "# for i in range(0,df_var.shape[0]):\n",
    "#     print(df_var['year'].iloc[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9432b58757fc97c8292661e5bdcc71590e5011f9"
   },
   "source": [
    "We have to see in how far a reduction of the data to one year or one grade is representative of the other years/grades in order to access the quality of our predictions.\nUnfortunately, we don't have consistent target data over the years. Maybe we can use one representative year in order to approximate all years? To do that, we should see if the school performance is constant over the years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check if constant school performance over years\n",
    "years = ['2013','2014','2015','2016']\n",
    "x = range(0,len(DBN_list))\n",
    "y = []\n",
    "z = []\n",
    "\n",
    "for i in range(0,len(DBN_list)):\n",
    "    df_var = (df_target[df_target['DBN'] == DBN_list[i]])\n",
    "    y.append(df_var['register_percentile'].max() - df_var['register_percentile'].min())\n",
    "    z.append(df_var['took_test_percentile'].max() - df_var['took_test_percentile'].min())\n",
    "    # print(str(round(var,3))+'    '+DBN_list[i])\n",
    "y.sort()\n",
    "z.sort()\n",
    "\n",
    "plt.scatter(x, y, label='Q(Registered)')\n",
    "plt.scatter(x, z, label='Q(Participated)')\n",
    "plt.xlabel('Arbitraty school number ', fontsize=16)\n",
    "plt.ylabel('Maximal percentile change ', fontsize=16)\n",
    "plt.legend(loc = \"lower right\", ncol=1, prop={'size':12})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f203ffcdd69ea79829a067ea75b19fa9dcb1a718"
   },
   "source": [
    "So apparently some schools have a very unsteady performance over the years. This means that the outcome of a year might not be representative of the general (time-averaged school peformance).\nWe can assume that the most recent year migh then be the more relevant, but further analysis of previous years might give additional insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "58e3bb54ca8168e408badc0e50b9813054114b9a"
   },
   "outputs": [],
   "source": [
    "# match features and targets for 2016\n",
    "DBN_list = df_target_init['DBN'].unique()\n",
    "df_features = df_features[df_features['DBN'].isin(DBN_list)]\n",
    "\n",
    "DBN_list = df_features['DBN'].unique()\n",
    "df_target = df_target_init.copy()\n",
    "df_target = df_target[df_target['year'] == 2016]\n",
    "df_target = df_target[df_target['DBN'].isin(DBN_list)]\n",
    "\n",
    "df_var = df_target.sort_values(by=['DBN'],ascending=True)\n",
    "# for i in range(0,df_var.shape[0]):\n",
    "#     print(str(df_var['DBN'].iloc[i])+'   '+str(df_var['grade'].iloc[i])+'   '+str(df_var['register_percentile'].iloc[i])+'   '+str(df_var['took_test_percentile'].iloc[i]))\n",
    "# still not fitting, due to grade inconsistencies\n",
    "print(df_features.shape)\n",
    "print(df_target.shape)\n",
    "# print(df_target.shape[0]/df_features.shape[0]*1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb1c874e1e9f1aa283f60648fe1cb1a8568b019c"
   },
   "source": [
    "How does the data vary for the grades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "1494717a713e36dfef5477918a0b59139c6a20e8"
   },
   "outputs": [],
   "source": [
    "# check if constant school performance over grades\n",
    "x = range(0,len(DBN_list))\n",
    "y = []\n",
    "z = []\n",
    "\n",
    "for i in range(0,len(DBN_list)):\n",
    "    df_var = (df_target[df_target['DBN'] == DBN_list[i]])\n",
    "    y.append(df_var['register_percentile'].max() - df_var['register_percentile'].min())\n",
    "    z.append(df_var['took_test_percentile'].max() - df_var['took_test_percentile'].min())\n",
    "\n",
    "y.sort()\n",
    "z.sort()\n",
    "\n",
    "plt.scatter(x, y, label='Q(Registered)')\n",
    "plt.scatter(x, z, label='Q(Participated)')\n",
    "plt.xlabel('Arbitraty school number ', fontsize=16)\n",
    "plt.ylabel('Maximal percentile change ', fontsize=16)\n",
    "plt.legend(loc = \"lower right\", ncol=1, prop={'size':12})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "43faac6baa678c5147db1bbe812f8c7d195045b0"
   },
   "outputs": [],
   "source": [
    "# Do all the schools have grade 8?\n",
    "print(list(df_target))\n",
    "df_target = df_target[df_target['grade'] == 8]\n",
    "# print(df_target['grade'])\n",
    "print(df_features.shape)\n",
    "print(df_target.shape)\n",
    "print(df_target.shape[0]/df_features.shape[0]*1.)\n",
    "\n",
    "# yes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d58b88126a678e240488d65a7fbd45d217d0c171"
   },
   "source": [
    "Let us see, if the variability in school performance over the years persists if we only consider the target data from grade 8th. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "139433b8f7d97d6d76e8c6c130ace82de3da0032"
   },
   "outputs": [],
   "source": [
    "# reload the data and select 8th graders\n",
    "df_features_init = file_to_df('../input/data-science-for-good/2016 School Explorer.csv')\n",
    "df_target_init = file_to_df('../input/augmented-d5-shsat-2/augmented_D5_SHSAT_Registrations_and_Testers.csv')\n",
    "\n",
    "# subset to numeric only plus school ID\n",
    "df_features_init.rename(columns={'Location Code':'DBN'}, inplace=True)\n",
    "colnames = list(df_features_init)\n",
    "\n",
    "df_features = pd.DataFrame()\n",
    "df_features['DBN'] = df_features_init['DBN']\n",
    "for i in range(0,len(colnames)):\n",
    "    if is_numeric_dtype(df_features_init[colnames[i]]):\n",
    "        df_features[colnames[i]] = df_features_init[colnames[i]]\n",
    "\n",
    "# match features and targets\n",
    "DBN_list = df_target_init['DBN'].unique()\n",
    "df_features = df_features[df_features['DBN'].isin(DBN_list)]\n",
    "\n",
    "DBN_list = df_features['DBN'].unique()\n",
    "df_target = df_target_init.copy()\n",
    "df_target = df_target[df_target['DBN'].isin(DBN_list)]\n",
    "\n",
    "# Select 8th graders\n",
    "df_target = df_target[df_target['grade'] == 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "cba4c5bc2e9d359e0bb8e4c1d2033495a64f6266"
   },
   "outputs": [],
   "source": [
    "# check if constant school performance over years\n",
    "years = ['2013','2014','2015','2016']\n",
    "x = range(0,len(DBN_list))\n",
    "y = []\n",
    "z = []\n",
    "\n",
    "for i in range(0,len(DBN_list)):\n",
    "    df_var = (df_target[df_target['DBN'] == DBN_list[i]])\n",
    "    y.append(df_var['register_percentile'].max() - df_var['register_percentile'].min())\n",
    "    z.append(df_var['took_test_percentile'].max() - df_var['took_test_percentile'].min())\n",
    "    # print(str(round(var,3))+'    '+DBN_list[i])\n",
    "y.sort()\n",
    "z.sort()\n",
    "\n",
    "plt.scatter(x, y, label='Q(Registered)')\n",
    "plt.scatter(x, z, label='Q(Participated)')\n",
    "plt.xlabel('Arbitraty school number ', fontsize=16)\n",
    "plt.ylabel('Maximal percentile change (8th)', fontsize=16)\n",
    "plt.legend(loc = \"lower right\", ncol=1, prop={'size':12})\n",
    "plt.show()\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_target.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc0802a02634d2d406651e996c9290792db0cde5"
   },
   "source": [
    "The variance in school performance persists over the years, even if we only select 8th graders. At this point, we can still just select the most recent year as this nicely leads to a one-to-one matching of features and targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "661a9f436509be6e1acf262a64800821799bd949"
   },
   "outputs": [],
   "source": [
    "# match features and targets for 2016 and 8th graders\n",
    "DBN_list = df_target_init['DBN'].unique()\n",
    "df_features = df_features[df_features['DBN'].isin(DBN_list)]\n",
    "\n",
    "DBN_list = df_features['DBN'].unique()\n",
    "df_target = df_target_init.copy()\n",
    "df_target = df_target[df_target['year'] == 2016]\n",
    "df_target = df_target[df_target['grade'] == 8]\n",
    "df_target = df_target[df_target['DBN'].isin(DBN_list)]\n",
    "\n",
    "df_var = df_target.sort_values(by=['DBN'],ascending=True)\n",
    "\n",
    "print(df_features.shape)\n",
    "print(df_target.shape)\n",
    "print(df_target.shape[0]/df_features.shape[0]*1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "6345cdb90c2708d9297324c1e990749f0908be62"
   },
   "outputs": [],
   "source": [
    "# match school id and sort row numbers\n",
    "df_target = df_target.sort_values(by=['DBN'],ascending=True)\n",
    "df_target = df_target.reset_index(drop=True)\n",
    "df_features = df_features.sort_values(by=['DBN'],ascending=True)\n",
    "df_features = df_features.reset_index(drop=True)\n",
    "\n",
    "# print(df_features.shape)\n",
    "# print(list(df_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "797ce6ac82fd1c18ff2d192876759b5cea00c900"
   },
   "source": [
    "So far we explored the data and selected the targets and features that can be consistently connected. The next step is to explore which features are relevant predictors for the school performance. As you see from the list above, the feature space is large. In order to address the relevance of features, we have to normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "3d5b1f4e8bff08d57f8c412bf27fa3695635ad3e"
   },
   "outputs": [],
   "source": [
    "# Step 1: Normalize test takers etc. to the number of students in the school\n",
    "df_var = df_features.copy()\n",
    "df_ft = df_features.copy()\n",
    "\n",
    "grades = [3,4,5,6,7,8]\n",
    "test_types = ['ELA','Math|math']\n",
    "for i in range(0,len(grades)):\n",
    "    for j in range(0,len(test_types)):\n",
    "        df_var = df_features.copy()\n",
    "        # grade\n",
    "        colnames = list(df_var.filter(regex=str(grades[i])+' ').columns)\n",
    "        df_var = df_var[colnames]\n",
    "    \n",
    "        # test type\n",
    "        colnames = list(df_var.filter(regex=test_types[j]).columns)\n",
    "        df_var = df_var[colnames]\n",
    "    \n",
    "        # number of pupils tested\n",
    "        all_colname = list(df_var.filter(regex='Tested|tested').columns)\n",
    "        df_all = df_var[all_colname]\n",
    "        df_var = df_var.drop(all_colname, axis=1)\n",
    "        colnames = list(df_var)\n",
    "    \n",
    "        # normalize\n",
    "        for col in range(0,len(colnames)):\n",
    "            df_var[colnames[col]] = df_var[colnames[col]]/df_all[all_colname[0]]*1.0\n",
    "        df_var = df_var.fillna(0)\n",
    "    \n",
    "        # substitute normalized results into original df\n",
    "        df_ft[colnames] = df_var[colnames]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "e42e312e65a7c261c203fd84f3324e6a0dacb076"
   },
   "outputs": [],
   "source": [
    "# Step 2: Now, standardize all features:\n",
    "df_var = df_ft.copy()\n",
    "df_var = df_var.drop(df_var[['DBN']], axis=1)\n",
    "feature_names = list(df_var.columns)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df_var = pd.DataFrame(scaler.fit_transform(df_var))\n",
    "df_var.columns = feature_names\n",
    "\n",
    "df_features = df_var\n",
    "# print(df_var.head(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "d4a553984888bab40f6c86a37dc5e50bb6bec55a"
   },
   "outputs": [],
   "source": [
    "# Step 3: Remove features with constant values\n",
    "def variance_threshold_select(df, thresh=0.0, na_replacement=-999):\n",
    "    df1 = df.copy(deep=True) # Make a deep copy of the dataframe\n",
    "    selector = VarianceThreshold(thresh)\n",
    "    selector.fit(df1.fillna(na_replacement)) # Fill NA values as VarianceThreshold cannot deal with those\n",
    "    df2 = df.loc[:,selector.get_support(indices=False)] # Get new dataframe with columns deleted that have NA values\n",
    "    return df2\n",
    "\n",
    "df_features = variance_threshold_select(df_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "795d0793738bad58151291686f3937b8b74a42ee"
   },
   "source": [
    "**Machine learning for descriptor relevance analysis**\n\nLet us now use LASSO to significanly reduce the feature space! We are dealing with a large p, medium n problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "6c54fc24ef249fafa914cdf74daaf0e08c541c4a"
   },
   "outputs": [],
   "source": [
    "# Pearson correlation of coefficients\n",
    "corr = df_features.corr(method='pearson')**2\n",
    "corr.columns = corr.columns.str.replace('_', ' ')\n",
    "corr.index = corr.index.str.replace('_', ' ')\n",
    "corr = corr.abs()\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr, cmap=\"Blues\", square=True)\n",
    "plt.title(r'Pearson correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bf14579fe26efeed6522fe5709096bedfa28242"
   },
   "source": [
    "We see  the vast feature space with a lot of correlation betweent he features! Our aim is to obtain a set of representative features which are not too correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "dcedf9e49f26a9af112b547fdf11ba6cd5d3d58c"
   },
   "outputs": [],
   "source": [
    "# Target: register_percentile\n",
    "X = df_features.iloc[:, 6:].astype(float)\n",
    "y = df_target.iloc[:, -2:].astype(float)\n",
    "y_idx = 0\n",
    "y = y.iloc[:,y_idx]\n",
    "print('Target: '+str(y.name))\n",
    "\n",
    "# Loop through regularization strength on a log scale and run LASSO.\n",
    "print('>>> Testing regularization strength')\n",
    "strength = 10 ** np.linspace(-3, 1., 7)\n",
    "strength.sort()\n",
    "for alpha in strength:\n",
    "    clf = linear_model.Lasso(alpha=alpha,max_iter=100000000)\n",
    "    clf.fit(X, y)\n",
    "    print(str('%.0E' % Decimal(alpha)), round(clf.score(X, y),3))\n",
    "    \n",
    "# Fit and predict with reasonable alpha\n",
    "alpha = 5E-01\n",
    "print('>>> LASSO with alpha = '+str(alpha))\n",
    "clf = linear_model.Lasso(alpha=alpha,max_iter=100000000)\n",
    "clf.fit(X, y)\n",
    "r_value = clf.score(X, y)\n",
    "y_hat = clf.predict(X)\n",
    "res = y_hat - y\n",
    "mae = np.mean(np.abs(res))\n",
    "\n",
    "# parity plot\n",
    "axmin = 0\n",
    "axmax = 100\n",
    "msg = \"$MAE$ = \" + str(round(mae,3)) + '\\n $R^2$ = ' + str(round(r_value, 3))\n",
    "plt.plot([-1, 130], [-1,130], color='black', lw=1.)\n",
    "plt.scatter(y_hat, y)\n",
    "plt.text(axmin*0.8+0.1*axmax, axmax*0.7, msg)\n",
    "plt.title('LASSO on registration percentiles', fontsize = 14)\n",
    "plt.xlabel(r'Predicted registration percentile ($\\hat{y}$)', fontsize=16)\n",
    "plt.ylabel(r'Actual registration percentile', fontsize=16)\n",
    "\n",
    "plt.xlim((0,100))\n",
    "plt.ylim((0,100))\n",
    "plt.show()\n",
    "\n",
    "# Descriptor relevance\n",
    "features = list(X)\n",
    "print(\"R2 = \" + str(round(clf.score(X, y),3)))\n",
    "coefficients = np.round(clf.coef_,3)\n",
    "\n",
    "coeff_dict = dict(zip(features, coefficients))\n",
    "coeff_rank = sorted(coeff_dict, key=lambda dict_key: abs(coeff_dict[dict_key]),reverse=True)\n",
    "best_features = []\n",
    "# for i in range(0,len(coeff_rank)):\n",
    "for i in range(0,20):\n",
    "    print(str(coeff_rank[i]+'    '+str(coeff_dict[coeff_rank[i]])))\n",
    "    best_features.append(str(coeff_rank[i]))\n",
    "\n",
    "best_num = 8\n",
    "best_features = best_features[0:best_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "474ef62e236ac8c602f3bb450d639f53bb1ec7ca"
   },
   "outputs": [],
   "source": [
    "# Manual feature selection\n",
    "X_best =  X[best_features]\n",
    "print(list(X_best))\n",
    "\n",
    "# Pearson correlation of coefficients\n",
    "corr = X_best.corr(method='pearson')**2\n",
    "corr.columns = corr.columns.str.replace('_', ' ')\n",
    "corr.index = corr.index.str.replace('_', ' ')\n",
    "corr = corr.abs()\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(corr, cmap=\"Blues\", square=True)\n",
    "plt.title(r'Pearson correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e1c41c235fbe1c3427436bfabc3edcf2aeaac1e"
   },
   "source": [
    "Amongst the best features selected by LASSO, there are still a couple with relatively high correlation. Of those, we may want to remove the once with the smallest coefficients. These are: 'Grade 4 ELA 4s - Black or African American' and  'Grade 3 Math 4s - All Students'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "cd5ad3f737f2276362b942c11aa9a45d43f85fc2"
   },
   "outputs": [],
   "source": [
    "# Manual feature selection\n",
    "best_features = ['Grade 7 Math 4s - Economically Disadvantaged', 'Grade 8 ELA - All Students Tested', 'Grade 6 Math 4s - Limited English Proficient',\n",
    "                 'Grade 3 Math 4s - Black or African American', 'Grade 7 ELA 4s - Black or African American',  'Grade 7 Math - All Students Tested']\n",
    "X_best =  X[best_features]\n",
    "print(list(X_best))\n",
    "\n",
    "# Linear regression\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(X_best, y)\n",
    "r_value = clf.score(X_best, y)\n",
    "y_hat = clf.predict(X_best)\n",
    "res = y_hat - y\n",
    "mae = np.mean(np.abs(res))\n",
    "\n",
    "# Parity plot\n",
    "msg = \"$MAE$ = \" + str(round(mae,3)) + '\\n $R^2$ = ' + str(round(r_value, 3))\n",
    "axmin = 0\n",
    "axmax = 100\n",
    "plt.plot([axmin,axmax], [axmin,axmax], color='black', lw=1.)\n",
    "plt.scatter(y_hat, y)\n",
    "plt.xlabel(r'Predicted value ($\\hat{y}$)')\n",
    "plt.ylabel(r'True value')\n",
    "plt.text(axmin*0.8+0.1*axmax, axmax*0.7, msg)\n",
    "plt.title('Plain least squares fitting with top '+str(best_num)+' descriptors', fontsize = 14)\n",
    "plt.xlim((axmin, axmax))\n",
    "plt.ylim((axmin, axmax))\n",
    "plt.show()\n",
    "\n",
    "# sensitivity analysis\n",
    "features = best_features\n",
    "print(\"R2 = \" + str(round(clf.score(X_best, y),3)))\n",
    "coefficients = np.round(clf.coef_,3)\n",
    "\n",
    "coeff_dict = dict(zip(features, coefficients))\n",
    "coeff_rank = sorted(coeff_dict, key=lambda dict_key: abs(coeff_dict[dict_key]),reverse=True)\n",
    "print('Coefficients: ')\n",
    "for i in range(0,len(coeff_rank)):\n",
    "    print(str(coeff_rank[i]+'    '+str(coeff_dict[coeff_rank[i]])))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "84a1a903c4191d4eca37f38a0041334372dd9fcb"
   },
   "outputs": [],
   "source": [
    "# Manual feature selection\n",
    "X_best =  X[best_features]\n",
    "print(list(X_best))\n",
    "\n",
    "# Pearson correlation of coefficients\n",
    "corr = X_best.corr(method='pearson')**2\n",
    "corr.columns = corr.columns.str.replace('_', ' ')\n",
    "corr.index = corr.index.str.replace('_', ' ')\n",
    "corr = corr.abs()\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(corr, cmap=\"Blues\", square=True)\n",
    "plt.title(r'Pearson correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3abc691c62b0818e45ec522b50a1b17e70bf577a"
   },
   "source": [
    "**Conclusion:**\n\nWe generated a hand full of descriptors which should be relevant for the prediciton of the performance of schools in terms of the number of students registering for the SHSAT test. \nThe results indicate that the performance in math of Economically Disadvantaged students and Black or African American students, already in lower grades, but especially in grade 7 and 8 is a good indicator for the number of registrations. \n\nThis can be interpreted in the following way: schools in which socio-economically disadvantages students and racial minorities are suuported to perform well have better chances of getting their students into the best high schools. **In turn, schools showing already low Math grades thoughout the grades (Grades 3-8) should be supported!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "934f04e0e4916fad562058b2abced1aa4a0cf92d"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
